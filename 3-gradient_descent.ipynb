{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc644765-70dd-43e7-bfc7-5940d5ba0cea",
   "metadata": {},
   "source": [
    "# **Gradient Descent Algorithm used to Optimize the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20cd171-ff9e-4ae9-8c2a-e6c2b61923e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366a8a7-b4ef-4662-9521-34a791dc9cc8",
   "metadata": {},
   "source": [
    "## **Part 1 : Gradient Descent From Strach (Using Numpy Only)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7d41be6-d4ec-426d-9f18-513186302778",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Our Function approximation is simple linear regressor which has the relationship,\n",
    "                Y = 2 * X\n",
    "'''\n",
    "\n",
    "X = np.array(\n",
    "            [1,2,3,4,5],\n",
    "            dtype = np.float32\n",
    "            )\n",
    "\n",
    "Y = np.array(\n",
    "            [2,4,6,8,10],\n",
    "            dtype = np.float32\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8f10bb4b-5bfc-4959-8adc-e5a896fc75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Forward Pass\n",
    "\n",
    "'''\n",
    "    Function Approximation : Y_hat = W * X\n",
    "    \n",
    "    dim(X) -> (5,) --> vector\n",
    "    dim(Y) -> (5,) --> vector\n",
    "    dim(W) -> ()   --> Scalar\n",
    "'''\n",
    "\n",
    "W = 0.0 #initialize weights to zero\n",
    "\n",
    "def forward_propogation(X):\n",
    "    return W * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94e75249-70f9-41ac-87d8-3113aa9369da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Loss Function\n",
    "\n",
    "'''\n",
    "    Loss Function approximated by,\n",
    "    \n",
    "          J = ((Y_hat - Y) ** 2).mean()\n",
    "'''\n",
    "\n",
    "def MSE(W , X , Y):\n",
    "    return ((W * X - Y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5443b2ed-b01c-4382-869f-6175286c8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Backward Pass\n",
    "\n",
    "'''\n",
    "        J = ((Y_hat - Y) ** 2).mean()\n",
    "        J = ((W * X - Y) ** 2) / N     ; Here N=5\n",
    "        dJ/dW = (2/N) * (Y_hat - Y) * X\n",
    "'''\n",
    "\n",
    "def gradients(X, Y):\n",
    "    Y_hat = forward_propogation(X)\n",
    "    return 2* np.dot(Y_hat - Y, X).mean()\n",
    "    \n",
    "def gradient_descent(W, dW, lr):\n",
    "    W = W - lr * dW\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40dcf243-36e2-41f3-858e-cd4a6f2166d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 -> Loss : 44.0 -> W : 2.2\n",
      "Epoch : 1 -> Loss : 0.4399999976158142 -> W : 1.98\n",
      "Epoch : 2 -> Loss : 0.004000000189989805 -> W : 2.002\n",
      "Epoch : 3 -> Loss : 0.0 -> W : 2.0\n",
      "Epoch : 4 -> Loss : 0.0 -> W : 2.0\n",
      "Epoch : 5 -> Loss : 0.0 -> W : 2.0\n",
      "Epoch : 6 -> Loss : 0.0 -> W : 2.0\n",
      "Epoch : 7 -> Loss : 0.0 -> W : 2.0\n",
      "Epoch : 8 -> Loss : 0.0 -> W : 2.0\n",
      "Epoch : 9 -> Loss : 0.0 -> W : 2.0\n"
     ]
    }
   ],
   "source": [
    "# Step 4 : Training Loop\n",
    "\n",
    "lr = 0.01\n",
    "epoches = 10\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    Y_hat = forward_propogation(X)\n",
    "    mse = MSE(W , X , Y)\n",
    "    dW = gradients(X, Y)\n",
    "    W = gradient_descent(W, dW, lr)\n",
    "    \n",
    "    print(\"Epoch : {} -> Loss : {} -> W : {}\".format(epoch, round(mse, 3), round(W, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313abe1-309b-48d9-82cf-1b11c3a8b9d4",
   "metadata": {},
   "source": [
    "# **Part 2 : Gradient Descent From AutoGrad (Using Torch Only)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0211848e-f1a6-476a-8eae-1cff656e427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(\n",
    "            [1,2,3,4,5],\n",
    "            dtype = torch.float32\n",
    "            )\n",
    "\n",
    "Y = torch.tensor(\n",
    "            [2,4,6,8,10],\n",
    "            dtype = torch.float32\n",
    "            )\n",
    "\n",
    "W = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91e38b93-16c2-4c34-9315-5b201f2269fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 -> Loss : 44.0 -> W : 0.44\n",
      "Epoch : 1 -> Loss : 26.77 -> W : 0.783\n",
      "Epoch : 2 -> Loss : 16.287 -> W : 1.051\n",
      "Epoch : 3 -> Loss : 9.909 -> W : 1.26\n",
      "Epoch : 4 -> Loss : 6.029 -> W : 1.423\n",
      "Epoch : 5 -> Loss : 3.668 -> W : 1.55\n",
      "Epoch : 6 -> Loss : 2.231 -> W : 1.649\n",
      "Epoch : 7 -> Loss : 1.358 -> W : 1.726\n",
      "Epoch : 8 -> Loss : 0.826 -> W : 1.786\n",
      "Epoch : 9 -> Loss : 0.503 -> W : 1.833\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "epoches = 10\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    \n",
    "    Y_hat = forward_propogation(X)\n",
    "    mse = MSE(W , X , Y)\n",
    "    mse.backward()\n",
    "\n",
    "    #updating weights shouldn't track the gradients (This isn't a part of the computational graph)\n",
    "    with torch.no_grad():\n",
    "        W.sub_(W.grad*lr)\n",
    "        W.grad.zero_() # avoid gradient accumilation\n",
    "        \n",
    "    print(\"Epoch : {} -> Loss : {} -> W : {}\".format(epoch, round(mse.item(), 3), round(W.item(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf319d-6b25-4f39-a561-a4021fe8fe5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

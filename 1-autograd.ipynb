{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03ea52e-36d2-4473-831f-b3b432436a11",
   "metadata": {},
   "source": [
    "## **Gradients used to optimize the function approximation for DNNs using Backpropogation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4a3a58-2fe1-41cb-bfdb-ffe9ceacc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d91f4-1944-4a86-8b75-092f3caef751",
   "metadata": {},
   "source": [
    "**Let's Define a tensor. In-default requires_grad argument is set to False. In order to calculate gradients we should enable this (requires_grad = True)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa45e5f8-4102-49c4-ad09-a08297d8f34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_without_grad = torch.randn(3)\n",
    "x_without_grad.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af1e1059-dc5d-4477-a321-f0a738faa9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function in order to create a computational graph \n",
    "\n",
    "def compGraph(x):\n",
    "    y = x + 2\n",
    "    z = y*y*2\n",
    "    z = z.mean()\n",
    "    return y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dc115f0-47e9-47ec-baaf-4121ecfaeb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_without_grad,z_without_grad = compGraph(x_without_grad) \n",
    "y_without_grad.requires_grad, z_without_grad.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "207396e5-9d05-47fb-98fc-1210a824d6dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4480\\1765909065.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz_without_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\zuu\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\zuu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "z_without_grad.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94ebb1-06c3-4275-afce-17bc373ae594",
   "metadata": {},
   "source": [
    "**RuntimeError Generated since we didn't enabled the requires_grad Flag. Let's redo the process by enabling that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1543ebc4-7a3b-4f88-b78c-bb5a0c6a1f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_with_grad = torch.randn(3, requires_grad=True)\n",
    "x_with_grad.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "495cb5e6-42bc-4e45-a4fd-3773a31aba43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_with_grad,z_with_grad = compGraph(x_with_grad) \n",
    "y_with_grad.requires_grad, z_with_grad.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "74777026-f43f-4b2b-82f3-75c4109179a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_with_grad.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20db683a-1544-4734-94f7-a38401c00965",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_with_grad.grad #return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80240b58-3542-41dd-922f-aa7607b03723",
   "metadata": {},
   "source": [
    "**But one thing to notice is that the concept of leaf variables and intermediate variables. if you call y_with_grad.grad the output would be None. The reason is that Y is an intermediate variable while X is a leaf variable & Z is the root variable. All the grads for intermendiate variables will be removed after calling backward() function. So if you want to retain gradients for intermediate variables then you should call y.retain_grad() before calling backward()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55bfdda4-8e57-48bc-af83-2569d82f978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_with_grad = torch.randn(3, requires_grad=True)\n",
    "y_with_grad,z_with_grad = compGraph(x_with_grad) \n",
    "y_with_grad.retain_grad()\n",
    "z_with_grad.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03f9d241-d23c-4b30-8b7d-74fe9d316256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2461, 4.3175, 1.7098])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_with_grad.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911a100-2615-4bcf-b7bf-7d9c85e4e480",
   "metadata": {},
   "source": [
    "## **Now learn how we can stop tracking the gradients**\n",
    "\n",
    "**There are 3 basic methods to perform this**\n",
    "\n",
    "        1) x.requires_grad_(False)\n",
    "        2) x.detach()\n",
    "        3) with torch.no_grad():\n",
    "                {erform opration with x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9482f2d7-1717-4e79-83b9-1ecb8aaae650",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(3, requires_grad=True)\n",
    "x2 = torch.randn(3, requires_grad=True)\n",
    "x3 = torch.randn(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed06f4-4489-46c7-b1fb-b600249bb081",
   "metadata": {},
   "source": [
    "## **Method 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c5e8e90-ac4a-449d-b72d-20984df13192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Grad Disabling :  tensor([0.2599, 1.0128, 0.2167], requires_grad=True)\n",
      "After Grad Disabling  :  tensor([0.2599, 1.0128, 0.2167])\n"
     ]
    }
   ],
   "source": [
    "print('Before Grad Disabling : ', x1)\n",
    "\n",
    "x1.requires_grad_(False) # when a function contain 'trailing _' which literally means it changes the variable inplace (overwrite)\n",
    "\n",
    "print('After Grad Disabling  : ', x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9498545-d965-45d8-9536-1ddf4163bb9e",
   "metadata": {},
   "source": [
    "## **Method 02**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03a21f3c-da0c-4465-9123-baa3abbb7780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Grad Disabling :  tensor([-1.5315,  1.4275,  0.0762], requires_grad=True)\n",
      "After Grad Disabling  :  tensor([-1.5315,  1.4275,  0.0762])\n"
     ]
    }
   ],
   "source": [
    "print('Before Grad Disabling : ', x2)\n",
    "\n",
    "x2 = x2.detach()\n",
    "\n",
    "print('After Grad Disabling  : ', x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358a627-fcfb-469e-8b15-170b2290eb99",
   "metadata": {},
   "source": [
    "## **Method 03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2813475-1ccc-4fb9-bcc7-9fff8860617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Grad Disabling :  tensor([-0.0487, -0.8465, -0.3214], requires_grad=True)\n",
      "After Grad Disabling  :  tensor([2.9513, 2.1535, 2.6786])\n"
     ]
    }
   ],
   "source": [
    "print('Before Grad Disabling : ', x3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y3 = x3 + 3\n",
    "    print('After Grad Disabling  : ', y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07a841-fe80-4ea0-99bf-aa689d94c7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
